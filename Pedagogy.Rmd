---
title: "Pedagogy"
author: "Nick Hass"
date: "2/17/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

Primary Research Questions

The goal of gathering the data was to assess if the semester learning activities (e.g. homework, quizzes andexams) was associated with successful learning of the course material (as measured by the final exam score).Specifically, the department researchers want to answer the following questions:

1.What activities, if any, are associated with improved learning?  Are there activities that are not associated with improved learning?
-- Which predictors have significantly positive betas? (hypothesis test, conf int).
-- Which predictors do not have significantly positive betas?

2.Of those activities that are associated with improved learning, which have the strongest effect on learning? How large are these effects?
-- Which of the predictors has the greatest betas? What is that beta and its interpretation?

3. How well do the class activities explain student learning?
-- What is the r-squared? Is it high?

4. Historically, were there any semesters that had either better or worse student learning than average?
-- Are there y outliers in the data set?

```{r}
scores <- read.csv("ClassAssessment.txt", sep = " ")
summary(scores)
#scores$Semester <- as.factor(scores$Semester)
#scores
```

EDA
```{r}
ggplot(data = scores, mapping = aes(x = Exam3+Exam2+Exam1, y = Final)) +
  geom_point() + geom_smooth()

ggplot(data = scores, mapping = aes(x = Exam1, y = Final)) +
  geom_point() + geom_smooth()

ggplot(data = scores, mapping = aes(x = Exam2, y = Final)) +
  geom_point() + geom_smooth()

ggplot(data = scores, mapping = aes(x = Exam3, y = Final)) +
  geom_point() + geom_smooth()

ggplot(data = scores, mapping = aes(x = HW, y = Final)) +
  geom_point() + geom_smooth()

ggplot(data = scores, mapping = aes(x = Quiz, y = Final)) +
  geom_point() + geom_smooth()

ggplot(data = scores) +
  geom_density(mapping = aes(x=Final))

ggplot(data = scores) +
  geom_point(mapping = aes(x=NStudents, y = Final))
```


```{r}
scores.lm <- lm(Final ~ ., data = scores)
summary.lm(scores.lm)
```
It looks like Exam 1, 2, 3, and HW are associated with improved learning.
This model as a whole (looking at R-squared), explains learning very well with R-squared=0.9121

```{r}
scores.lm2 <- lm(Final ~ Exam1 + Exam2 + Exam3 + HW, data = scores)
summary(scores.lm2)
plot(scores.lm2)
```

This reduced model outperforms the full model because the adjusted R-squared is higher with a 0.8939.

Model Validation
LINE
Linearity - Yes
Independence - Probably no, but assume yes.
Normality - yes
Equal variance - No


```{r}
# Fit heteroskedastic model
library(nlme)
scores.gls <- nlme::gls(model = Final ~ Exam1 + Exam2 + Exam3 + HW + Quiz + Semester, 
    data = scores, 
    weights = varFixed(value = ~NStudents),
    method = "ML") # Maximum Likelihood

summary(scores.gls)
GGally::ggpairs(scores)
```

$\text{EatingOut} \sim N(\text{Income}\beta, \sigma^2{\bf D}(\theta)) \ \ \ \  d_{ii}=e^{2log(\text{income}_i)\theta}$  


$y = X\beta + \epsilon$

$\epsilon \sim N(0, \sigma^2I)$


Model Validation

Linearity
```{r}
car::avPlots(scores.lm2)
```
Linearity holds.

Independence - I am assuming that there is independence between observations.

Normality
```{r}
ks.test(resid(object=scores.gls, type="pearson"), "pnorm")
# High p-value means that there is evidence for it being normal.
```

Normality assumption holds.

Equal Variance
```{r}
ggplot() + 
  geom_point(mapping = aes(x = fitted(scores.gls), y = new_resid))
```

Equal variance holds.


Cross Validation

```{r}
source("glstools-master/predictgls.R")

n.cv <- 100 #Number of CV studies to run
n.test <- floor(dim(scores)[1]*.2) #Number of observations in a test set 
rpmse <- rep(x = NA, times = n.cv)
bias <- rep(x = NA, times = n.cv)
wid <- rep(x = NA, times = n.cv)
cvg <- rep(x = NA, times = n.cv)
for(cv in 1:n.cv) {
  ## Select test observations
  test.obs <- sample(x = 1:(dim(scores)[1]), size = n.test)
  
  ## Split into test and training sets
  test.set <- scores[test.obs,]
  train.set <- scores[-test.obs,]
  
  ## Fit a model using the training data
  scores.gls <- nlme::gls(model = Final ~ Exam1 + Exam2 + Exam3 + HW + Quiz + Semester, 
    data = train.set, 
    weights = varFixed(value = ~NStudents),
    method = "ML") # Maximum Likelihood
  
  ## Generate predictions for the test set
  my.preds <- predictgls(nlme::gls(model = Final ~ Exam1 + Exam2 + Exam3 + HW + Quiz + Semester, 
    data = train.set, 
    weights = varFixed(value = ~NStudents),
    method="ML"),
    newdframe = test.set,
    level = 0.95)
  
  ## Calculate bias
  bias[cv] <- mean(my.preds[,'Prediction'] - test.set[['Final']])
  
  ## Calculate RPMSE
  rpmse[cv] <- (test.set[['Final']] - my.preds[,'Prediction'])^2 %>% mean() %>% sqrt()
  
  ## Calculate Coverage
  cvg[cv] <- ((test.set[['Final']] > my.preds[,'lwr']) & (test.set[['Final']] < my.preds[,'upr'])) %>% mean()
  
  ## Calculate Width
  wid[cv] <- (my.preds[,'upr'] - my.preds[,'lwr']) %>% mean()
  
}
```

```{r}
mean(bias)
mean(rpmse)
mean(cvg)
mean(wid)
```

Average Bias: 0.02747208
Average RPMSE: 0.4897444
Average Coverage: 0.91
Average Width: 2.06205

```{r}
score_predictions <- predictgls(nlme::gls(model = Final ~ Exam1 + Exam2 + Exam3 + HW + Quiz + Semester, 
    data = scores, 
    weights = varFixed(value = ~NStudents),
    method="ML"),
    newdframe = scores,
    level = 0.95)
```

```{r}
summary(scores.gls)

n <- dim(scores)[1]
se <- summary(scores.gls)$tTable[6,2]
bta <- coef(scores.gls)[6]
test.stat <- (bta-0)/se
pvalue <- pt(test.stat, df=n-(6+1), lower.tail=FALSE)
# Since p-value of 0.159 is higher than our alpha of 0.05, we conclude that Quizzes do not impact learning.
```

The activities that have a positive association with improved learning are Exam 1, Exam 2, Exam 3, and HW. Quiz scores do not have a statistically significant association to improve or decrease learning.

For every one point increase in Exam 1 score, the Final exam score will improve by 0.267 points on average. 

For every one point increase in Exam 2 score, the Final exam score will improve by 0.42469 points on average. 

For every one point increase in Exam 3 score, the Final exam score will improve by 0.44674 points on average. 

For every one point increase in HW score, the Final exam score will improve by 0.42635 points on average. 


```{r}
# r-squared
# 1 - ( sum((yi - y_hat)^2 ) / sum((yi - y)^2 ))
1 - ( sum((score_predictions$Final - score_predictions$Prediction)^2 ) / sum((score_predictions$Final - mean(score_predictions$Final))^2 ))
```

R-Square: 0.9112851


Did you identify any semesters that were better or worse in terms of student learning? If so, which and by how much?
```{r}
score_predictions
```

